{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27e91147",
   "metadata": {},
   "source": [
    "https://ai.google.dev/gemma/docs/core/huggingface_text_full_finetune\n",
    "\n",
    "https://huggingface.co/collections/google/gemma-3-release\n",
    "\n",
    "https://huggingface.co/datasets/bebechien/MobileGameNPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a647716",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "from random import randint\n",
    "import re\n",
    "\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b9b9bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['timestamp', 'comment_id', 'comment_body', 'parent_text']\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\n",
    "    \"csv\", name=\"social-media-conversations\", split=\"train\", data_files=\"conversations.csv\"\n",
    ")\n",
    "dataset = dataset.shuffle(seed=42)\n",
    "print(dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f421c428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_conversation(sample):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": sample[\"parent_text\"]},\n",
    "            {\"role\": \"assistant\", \"content\": sample[\"comment_body\"]},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "# Convert dataset to conversational format\n",
    "dataset = dataset.map(format_conversation, remove_columns=dataset.features, batched=False)\n",
    "\n",
    "# Split dataset into 80% training samples and 20% test samples\n",
    "dataset = dataset.train_test_split(test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "320c03bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"content\": \"Yeah, at 65 a fatal heart attack, stroke, pulmonary embolism, etc. aren't out of the realm of possibility.\",\n",
      "    \"role\": \"user\"\n",
      "  },\n",
      "  {\n",
      "    \"content\": \"They aren't out of the realm of possibility at any age. It's just different levels of probability.\",\n",
      "    \"role\": \"assistant\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "n_example_train = 2\n",
    "# Print formatted user prompt\n",
    "print(json.dumps(dataset[\"train\"][n_example_train][\"messages\"], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0ae4a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"content\": \"600 mph is 965.61 km/h\",\n",
      "    \"role\": \"user\"\n",
      "  },\n",
      "  {\n",
      "    \"content\": \"good bot\",\n",
      "    \"role\": \"assistant\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "n_example_test = 3004\n",
    "print(json.dumps(dataset[\"test\"][n_example_test][\"messages\"], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f73750fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(dataset['train']): 29572\n",
      "len(dataset['test']): 7393\n"
     ]
    }
   ],
   "source": [
    "print(f\"len(dataset['train']): {len(dataset['train'])}\")\n",
    "print(f\"len(dataset['test']): {len(dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbb0cdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model = \"google/gemma-3-4b-it\"\n",
    "base_model = \"google/gemma-3-12b-it\"\n",
    "# base_model = \"google/gemma-3-27b-it\"\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "learning_rate = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f265b75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_dtype = torch.bfloat16\n",
    "\n",
    "# Define model init arguments\n",
    "model_kwargs = dict(\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    torch_dtype=torch_dtype,  # What torch dtype to use, defaults to auto\n",
    "    device_map=\"auto\",  # Let torch decide how to load the model\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model, **model_kwargs)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487c79f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Device: {model.device}\")\n",
    "print(f\"DType: {model.dtype}\")\n",
    "print(f\"model.config._attn_implementation: {model.config._attn_implementation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949b361c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and tokenizer into the pipeline\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Load a sample from the test dataset\n",
    "test_sample = dataset[\"test\"][n_example_test]\n",
    "\n",
    "# Convert a test example into a prompt with the Gemma template\n",
    "prompt = pipe.tokenizer.apply_chat_template(\n",
    "    test_sample[\"messages\"][:1], tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "outputs = pipe(prompt, max_new_tokens=256, disable_compile=True)\n",
    "\n",
    "# Extract the user query and original answer\n",
    "print(f\"Question:\\n{test_sample['messages'][0]['content']}\\n\")\n",
    "print(f\"Original Answer:\\n{test_sample['messages'][1]['content']}\\n\")\n",
    "print(f\"Generated Answer (base model):\\n{outputs[0]['generated_text'][len(prompt):].strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f780d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_dtype = model.dtype\n",
    "evaluations = 10\n",
    "eval_steps = len(dataset['train']) // evaluations\n",
    "\n",
    "args = SFTConfig(\n",
    "    output_dir=checkpoint_dir,  # directory to save and repository id\n",
    "    max_length=512,  # max sequence length for model and packing of the dataset\n",
    "    packing=True,  # Groups multiple samples in the dataset into a single sequence\n",
    "    num_train_epochs=1,  # number of training epochs\n",
    "    per_device_train_batch_size=1,  # batch size per device during training\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adamw_torch_fused\",  # use fused adamw optimizer\n",
    "    logging_steps=1,  # log every step\n",
    "    save_strategy=\"no\",  # save checkpoint never\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=eval_steps,\n",
    "    learning_rate=learning_rate,  # learning rate\n",
    "    fp16=True if torch_dtype == torch.float16 else False,  # use float16 precision\n",
    "    bf16=True if torch_dtype == torch.bfloat16 else False,  # use bfloat16 precision\n",
    "    lr_scheduler_type=\"constant\",  # use constant learning rate scheduler\n",
    "    push_to_hub=False,  # do not push model to hub\n",
    "    report_to=\"tensorboard\",  # report metrics to tensorboard\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False,  # Template with special tokens\n",
    "        \"append_concat_token\": True,  # Add EOS token as separator token between examples\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2fdc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Trainer object\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fa1703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if packing is enabled, the number of training steps is different\n",
    "# we need to set the eval_steps accordingly\n",
    "training_steps = trainer.args.num_train_epochs * len(trainer.get_train_dataloader())\n",
    "eval_steps = training_steps // evaluations\n",
    "trainer.args.eval_steps = eval_steps\n",
    "print(f\"eval_steps: {trainer.args.eval_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c6857c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training, the model will be automatically saved to the Hub and the output directory\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588a5e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model again to the Hugging Face Hub\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338e5419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the log history\n",
    "log_history = trainer.state.log_history\n",
    "\n",
    "# Extract training / validation loss\n",
    "train_losses = [log[\"loss\"] for log in log_history if \"loss\" in log]\n",
    "epoch_train = [log[\"epoch\"] for log in log_history if \"loss\" in log]\n",
    "eval_losses = [log[\"eval_loss\"] for log in log_history if \"eval_loss\" in log]\n",
    "epoch_eval = [log[\"epoch\"] for log in log_history if \"eval_loss\" in log]\n",
    "\n",
    "# Plot the training loss\n",
    "plt.plot(epoch_train, train_losses, label=\"Training Loss\")\n",
    "plt.plot(epoch_eval, eval_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss per Epoch\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cb9543",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, tokenizer, pipe, trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14708bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "/home/florin/git/FlorinAndrei/gemma3-full-finetune-social-media/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:283: UserWarning: \n",
      "    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.\n",
      "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "    (8.0) - (12.0)\n",
      "    \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "115d078a687d4c04abcbdae35858ef9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = checkpoint_dir\n",
    "\n",
    "# Load Model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, torch_dtype=\"auto\", device_map=\"auto\", attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Load the model and tokenizer into the pipeline\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7ccd6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "DType: torch.bfloat16\n",
      "model.config._attn_implementation: flash_attention_2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Device: {model.device}\")\n",
    "print(f\"DType: {model.dtype}\")\n",
    "print(f\"model.config._attn_implementation: {model.config._attn_implementation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77a622a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "What? Never knew this. Anyone know a more scientific term? \n",
      "\n",
      "Also, that page is a clusterfuck of lightboxes and popups.\n",
      "Original Answer:\n",
      "Except there is no \"decomposition\" going on. She's not rotting and stinking like a corpse. From a purely external perspective (ignoring the brain per se), this is not that different from a very deep coma.\n",
      "\n",
      "Very unfortunate choice of words in the title.\n",
      "Generated Answer:\n",
      "It's called diffraction.\n",
      "--------------------------------------------------------------------------------\n",
      "Question:\n",
      "_Striking_ and _arresting_!\n",
      "Original Answer:\n",
      "But hopefully not breath-taking.\n",
      "Generated Answer:\n",
      "It is a bit... shocking.\n",
      "--------------------------------------------------------------------------------\n",
      "Question:\n",
      "Not really. I live in Australia and getting surgery with a skilled specialist in these conditions has enormous wait lists.  I had my surgeries under private insurance and they still cost me a fortune because our government set $ for those surgery items are well under what they actually cost.\n",
      "Original Answer:\n",
      "Australia is catching the Murican Syndrome really fast now.\n",
      "\n",
      "Sorry, guys.\n",
      "Generated Answer:\n",
      "> our government set $ for those surgery items are well under what they actually cost\n",
      "\n",
      "That's not how it works in a civilized country.\n",
      "--------------------------------------------------------------------------------\n",
      "Question:\n",
      "The only people with real answers to that aren't going to be found on reddit.\n",
      "\n",
      "Either Putin is cleaning out traitors or perceived traitors or he's experiencing a coup.\n",
      "\n",
      "No matter how you look at it thats bad for Putin.\n",
      "\n",
      "On one hand it could mean he's got enough opposition he's being forced to knock a lot of pieces off the board. Or he's deranged enough he's knocking pieces off the board that weren't really a threat.\n",
      "\n",
      "Or he's losing his own loyal people and is closer to losing his seat.\n",
      "\n",
      "No matter how you slice it that spells bad things for Putin.\n",
      "Original Answer:\n",
      "> Either Putin is cleaning out traitors or perceived traitors or he's experiencing a coup.\n",
      "\n",
      "Maybe a bit of both.\n",
      "Generated Answer:\n",
      "> No matter how you look at it that spells bad things for Putin.\n",
      "\n",
      "Yeah, but that's the thing - it's not bad for him, it's good for the world.\n",
      "--------------------------------------------------------------------------------\n",
      "Question:\n",
      "What would happen if a similar or larger meteor were to crash directly into one of our polar ice caps? What kind of effect might it have on the ice, rising seas, etc? \n",
      "Original Answer:\n",
      "As big as this one? Maybe kill a couple very unlucky penguins.\n",
      "\n",
      "Bigger? It depends. Probably no effect on sea level, since the energy required for that is ginormous. It would produce mass destruction on Earth before it melted enough ice to cause widespread floods.\n",
      "Generated Answer:\n",
      "If it's large enough, it would melt the whole planet.\n",
      "\n",
      "If it's small enough, nothing will happen.\n",
      "\n",
      "There's a lot of ice in the polar caps. It's not like it's the whole Arctic ocean.\n",
      "--------------------------------------------------------------------------------\n",
      "Question:\n",
      "Can some scientific claims be rejected outright on philosophical grounds? Is philosophy the basis of all science? No amount of scientific proof can validate claims which involve things which are fundamentally impossible, and one should not even attempt to do so.\n",
      "Original Answer:\n",
      "The ultimate source of truth is the experiment. End of story.\n",
      "\n",
      "Ignore this warning, and watch yourself gradually slide into solipsism.\n",
      "Generated Answer:\n",
      "You seem to be thinking in terms of a either/or.\n",
      "\n",
      "This is not an either/or. There is no either/or.\n",
      "\n",
      "Science is the search for knowledge. Philosophy is the search for knowledge. They're two different ways of doing that.\n",
      "\n",
      "You can do science without being a philosopher. You can do philosophy without being a scientist.\n",
      "\n",
      "But you can do both.\n",
      "--------------------------------------------------------------------------------\n",
      "Question:\n",
      "I think the tree line has more to do with temperature.\n",
      "Original Answer:\n",
      "And precipitation.\n",
      "Generated Answer:\n",
      "It's also a matter of oxygen.\n",
      "--------------------------------------------------------------------------------\n",
      "Question:\n",
      "In the non-spherical models of the shape of the universe: Where does the light emitted by an object near its edge go?\n",
      "Please humor me, this might be a very stupid question.\n",
      "So apparently the shape of the universe is not agreed upon (according to wikipedia). \n",
      "\n",
      "In the spherical model there's no edge and everything makes perfect sense. Like flying across the globe, there's really no edge and no \"end\" to it.\n",
      "\n",
      "But what about the other models (flat, hyperbolic)? Let's say we have a star near their \"edge\", which emmits light. Where does this light go? What happens when it reaches the edge of the universe? How do these models deal with that question?\n",
      "\n",
      "Is my understanding of the flat and hyperbolic models fundamentally flawed?\n",
      "\n",
      "\n",
      "\n",
      "Original Answer:\n",
      "I don't think there is any model of the Universe that has \"edges\". Everything I've seen is either infinite, or boundless (looped into itself).\n",
      "Generated Answer:\n",
      "> Where does this light go?\n",
      "\n",
      "It goes where it goes, and that's the answer.\n",
      "\n",
      "> But what about the other models (flat, hyperbolic)?\n",
      "\n",
      "They don't have an edge.\n",
      "\n",
      "> Where does this light go?\n",
      "\n",
      "It goes where it goes.\n",
      "\n",
      "> How do these models deal with that question?\n",
      "\n",
      "They don't.\n",
      "\n",
      "> Is my understanding of the flat and hyperbolic models fundamentally flawed?\n",
      "\n",
      "You're trying to extrapolate conclusions that do not exist in those models. They are all consistent with the Big Bang theory, and they are all consistent with the observable universe.\n",
      "\n",
      "Some of them may or may not be consistent with the rest of the universe, which is unobservable.\n",
      "\n",
      "The spherical model is consistent with the Big Bang, but it requires the universe to be finite in size. It also requires the universe to be finite in age.\n",
      "\n",
      "The other two models are consistent with the Big Bang, but they are consistent with the universe being infinite in size, and infinite in age.\n",
      "\n",
      "All of these models are consistent with the observable universe.\n",
      "\n",
      "The real issue is: which one is the right one? We don't know.\n",
      "--------------------------------------------------------------------------------\n",
      "Question:\n",
      ">Some people just don't like new things.\n",
      "\n",
      "Typically they are called \"conservatives.\"\n",
      "Original Answer:\n",
      "*regressives*\n",
      "Generated Answer:\n",
      "> Typically they are called \"conservatives.\"\n",
      "\n",
      "And when I was a kid, they were called \"old people\".\n",
      "\n",
      "I'm not saying it's the same thing, I'm just saying - if you're old enough, the old folks are the ones who will be in the way.\n",
      "--------------------------------------------------------------------------------\n",
      "Question:\n",
      "How does Meals on Wheels not \"work?\" Isn't the entire point of it just to deliver food to elderly and disabled people?\n",
      "Original Answer:\n",
      "Yeah, but then those people are a burden. That's why the thing is not working.\n",
      "\n",
      "/s, kinda\n",
      "Generated Answer:\n",
      ">Isn't the entire point of it just to deliver food to elderly and disabled people?\n",
      "\n",
      "Not anymore, apparently.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def test(test_sample):\n",
    "    # Extract the user query and original answer\n",
    "    print(f\"Question:\\n{test_sample['messages'][0]['content']}\")\n",
    "    print(f\"Original Answer:\\n{test_sample['messages'][1]['content']}\")\n",
    "\n",
    "    # Convert a test example into a prompt with the Gemma template\n",
    "    prompt = pipe.tokenizer.apply_chat_template(\n",
    "        test_sample[\"messages\"][:1], tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    outputs = pipe(prompt, max_new_tokens=256, disable_compile=True)\n",
    "    print(f\"Generated Answer:\\n{outputs[0]['generated_text'][len(prompt):].strip()}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "\n",
    "# Test with an unseen dataset\n",
    "for i in range(10):\n",
    "    n_test = randint(0, len(dataset['test']) - 1)\n",
    "    test(dataset['test'][n_test])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
