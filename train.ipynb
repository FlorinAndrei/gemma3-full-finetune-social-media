{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27e91147",
   "metadata": {},
   "source": [
    "https://ai.google.dev/gemma/docs/core/huggingface_text_full_finetune\n",
    "\n",
    "https://huggingface.co/collections/google/gemma-3-release\n",
    "\n",
    "https://huggingface.co/datasets/bebechien/MobileGameNPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a647716",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/florin/git/FlorinAndrei/gemma3-full-finetune-social-media/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:283: UserWarning: \n",
      "    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.\n",
      "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "    (8.0) - (12.0)\n",
      "    \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "from random import randint\n",
    "import re\n",
    "\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b27205f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conversation(sample):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": sample[\"player\"]},\n",
    "            {\"role\": \"assistant\", \"content\": sample[\"alien\"]},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "npc_type = \"martian\"\n",
    "\n",
    "# Load dataset from the Hub\n",
    "dataset = load_dataset(\"bebechien/MobileGameNPC\", npc_type, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abe92d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'category', 'player', 'alien']\n",
      "{\n",
      "  \"id\": 5,\n",
      "  \"category\": \"Greetings & Basic Questions\",\n",
      "  \"player\": \"Goodbye.\",\n",
      "  \"alien\": \"Vorp-vorp, Earth-kin. May your zky stay blue.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(dataset.column_names)\n",
    "print(json.dumps(dataset[4], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6633f8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataset to conversational format\n",
    "dataset = dataset.map(create_conversation, remove_columns=dataset.features, batched=False)\n",
    "\n",
    "# Split dataset into 80% training samples and 20% test samples\n",
    "dataset = dataset.train_test_split(test_size=0.2, shuffle=False)\n",
    "\n",
    "# Print formatted user prompt\n",
    "print(json.dumps(dataset[\"train\"][0][\"messages\"], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b9b9bdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "869541aebb6c424b8c87e15f0c936f23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['timestamp', 'comment_id', 'comment_body', 'parent_text']\n",
      "{\n",
      "  \"timestamp\": \"2021-08-25 16:32:18 UTC\",\n",
      "  \"comment_id\": \"hab6nka\",\n",
      "  \"comment_body\": \"Pfft, just turn the volume way up, it'll blow dry it.\",\n",
      "  \"parent_text\": \"Can confirm, shoulder length hair is not conducive to wearing headphones for the 4 hours after I shower.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "n_example = 4\n",
    "\n",
    "dataset2 = load_dataset(\n",
    "    \"csv\", name=\"social-media-conversations\", split=\"train\", data_files=\"conversations.csv\"\n",
    ")\n",
    "dataset2 = dataset2.shuffle(seed=42)\n",
    "print(dataset2.column_names)\n",
    "print(json.dumps(dataset2[n_example], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbb0cdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"google/gemma-3-4b-it\"\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "learning_rate = 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f265b75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5559b40c2c68422a9356c556d5935295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "DType: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "torch_dtype = torch.bfloat16\n",
    "\n",
    "# Define model init arguments\n",
    "model_kwargs = dict(\n",
    "    attn_implementation=\"eager\",  # Use \"flash_attention_2\" when running on Ampere or newer GPU\n",
    "    torch_dtype=torch_dtype,  # What torch dtype to use, defaults to auto\n",
    "    device_map=\"auto\",  # Let torch decide how to load the model\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model, **model_kwargs)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "\n",
    "print(f\"Device: {model.device}\")\n",
    "print(f\"DType: {model.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39ed432d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma3ForConditionalGeneration(\n",
      "  (model): Gemma3Model(\n",
      "    (vision_tower): SiglipVisionModel(\n",
      "      (vision_model): SiglipVisionTransformer(\n",
      "        (embeddings): SiglipVisionEmbeddings(\n",
      "          (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
      "          (position_embedding): Embedding(4096, 1152)\n",
      "        )\n",
      "        (encoder): SiglipEncoder(\n",
      "          (layers): ModuleList(\n",
      "            (0-26): 27 x SiglipEncoderLayer(\n",
      "              (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "              (self_attn): SiglipAttention(\n",
      "                (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
      "                (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
      "                (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
      "                (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
      "              )\n",
      "              (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): SiglipMLP(\n",
      "                (activation_fn): GELUTanh()\n",
      "                (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
      "                (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (multi_modal_projector): Gemma3MultiModalProjector(\n",
      "      (mm_soft_emb_norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
      "      (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
      "    )\n",
      "    (language_model): Gemma3TextModel(\n",
      "      (embed_tokens): Gemma3TextScaledWordEmbedding(262208, 2560, padding_idx=0)\n",
      "      (layers): ModuleList(\n",
      "        (0-33): 34 x Gemma3DecoderLayer(\n",
      "          (self_attn): Gemma3Attention(\n",
      "            (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
      "            (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
      "            (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
      "            (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
      "            (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "            (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "          )\n",
      "          (mlp): Gemma3MLP(\n",
      "            (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
      "            (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
      "            (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
      "            (act_fn): GELUTanh()\n",
      "          )\n",
      "          (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "          (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "          (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "          (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "        )\n",
      "      )\n",
      "      (norm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "      (rotary_emb): Gemma3RotaryEmbedding()\n",
      "      (rotary_emb_local): Gemma3RotaryEmbedding()\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2560, out_features=262208, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949b361c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "(Stands idle for too long)\n",
      "\n",
      "Original Answer:\n",
      "You'z broken, Terran? Or iz diz... 'meditation'? You look like you're trying to lay an egg.\n",
      "\n",
      "Generated Answer (base model):\n",
      "Okay, you've been standing idle for a while! That's perfectly alright. It happens to the best of us. \n",
      "\n",
      "It's a good opportunity to just *be* for a moment. \n",
      "\n",
      "Is there anything you'd like to do? Here are some ideas, or we can just let the silence hang for a bit:\n",
      "\n",
      "*   **Talk about something:** Do you want to chat about anything at all?  A hobby, a recent event, a question you have, anything?\n",
      "*   **Play a game:** I can play a simple game with you like 20 questions, a number guessing game, or even just tell you a short story.\n",
      "*   **Ask me a question:**  Do you have any questions for me about anything?\n",
      "*   **Just be quiet:** Sometimes, silence is nice too.  If you just want to be present, that's perfectly fine.\n",
      "\n",
      "Just let me know what you'd like to do, or if you'd just like me to wait a little while longer.\n"
     ]
    }
   ],
   "source": [
    "# Load the model and tokenizer into the pipeline\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Load a random sample from the test dataset\n",
    "rand_idx = randint(0, len(dataset[\"test\"]) - 1)\n",
    "test_sample = dataset[\"test\"][rand_idx]\n",
    "\n",
    "# Convert as test example into a prompt with the Gemma template\n",
    "prompt = pipe.tokenizer.apply_chat_template(\n",
    "    test_sample[\"messages\"][:1], tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "outputs = pipe(prompt, max_new_tokens=256, disable_compile=True)\n",
    "\n",
    "# Extract the user query and original answer\n",
    "print(f\"Question:\\n{test_sample['messages'][0]['content']}\\n\")\n",
    "print(f\"Original Answer:\\n{test_sample['messages'][1]['content']}\\n\")\n",
    "print(f\"Generated Answer (base model):\\n{outputs[0]['generated_text'][len(prompt):].strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f780d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_dtype = model.dtype\n",
    "\n",
    "args = SFTConfig(\n",
    "    output_dir=checkpoint_dir,  # directory to save and repository id\n",
    "    max_length=512,  # max sequence length for model and packing of the dataset\n",
    "    packing=False,  # Groups multiple samples in the dataset into a single sequence\n",
    "    num_train_epochs=5,  # number of training epochs\n",
    "    per_device_train_batch_size=1,  # batch size per device during training\n",
    "    gradient_checkpointing=False,  # Caching is incompatible with gradient checkpointing\n",
    "    optim=\"adamw_torch_fused\",  # use fused adamw optimizer\n",
    "    logging_steps=1,  # log every step\n",
    "    save_strategy=\"no\",  # save checkpoint never\n",
    "    eval_strategy=\"epoch\",  # evaluate checkpoint every epoch\n",
    "    learning_rate=learning_rate,  # learning rate\n",
    "    fp16=True if torch_dtype == torch.float16 else False,  # use float16 precision\n",
    "    bf16=True if torch_dtype == torch.bfloat16 else False,  # use bfloat16 precision\n",
    "    lr_scheduler_type=\"constant\",  # use constant learning rate scheduler\n",
    "    push_to_hub=False,  # do not push model to hub\n",
    "    report_to=\"tensorboard\",  # report metrics to tensorboard\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False,  # Template with special tokens\n",
    "        \"append_concat_token\": True,  # Add EOS token as separator token between examples\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d2fdc94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68369953fa9744b9ab412e99e4638a55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ec063dc80534722bb9585e476789364",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98569deb80d84be1a79bf6b425a0d17c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "197677fb46b84c698c9465f0870c13fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "You cannot perform fine-tuning on purely quantized models. Please attach trainable adapters on top of the quantized model to correctly perform fine-tuning. Please see: https://huggingface.co/docs/transformers/peft for more details",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Create Trainer object\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m trainer = \u001b[43mSFTTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtest\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/FlorinAndrei/gemma3-full-finetune-social-media/.venv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:844\u001b[39m, in \u001b[36mSFTTrainer.__init__\u001b[39m\u001b[34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics, peft_config, formatting_func)\u001b[39m\n\u001b[32m    836\u001b[39m \u001b[38;5;28mself\u001b[39m._total_train_tokens = \u001b[32m0\u001b[39m\n\u001b[32m    838\u001b[39m \u001b[38;5;66;03m# Initialize the Trainer. Parent class will handle:\u001b[39;00m\n\u001b[32m    839\u001b[39m \u001b[38;5;66;03m# - DeepSpeed configuration (through create_accelerator_and_postprocess)\u001b[39;00m\n\u001b[32m    840\u001b[39m \u001b[38;5;66;03m# - FSDP setup\u001b[39;00m\n\u001b[32m    841\u001b[39m \u001b[38;5;66;03m# - Distributed training setup\u001b[39;00m\n\u001b[32m    842\u001b[39m \u001b[38;5;66;03m# - Optimizer and scheduler creation\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m844\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    846\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    848\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    849\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    850\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    851\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompute_loss_func\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompute_loss_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    852\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    853\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    854\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    855\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer_cls_and_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer_cls_and_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    856\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreprocess_logits_for_metrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreprocess_logits_for_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    857\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    859\u001b[39m \u001b[38;5;66;03m# Initialize activation offloading context\u001b[39;00m\n\u001b[32m    860\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.activation_offloading:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/FlorinAndrei/gemma3-full-finetune-social-media/.venv/lib/python3.12/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/FlorinAndrei/gemma3-full-finetune-social-media/.venv/lib/python3.12/site-packages/transformers/trainer.py:558\u001b[39m, in \u001b[36mTrainer.__init__\u001b[39m\u001b[34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics)\u001b[39m\n\u001b[32m    556\u001b[39m \u001b[38;5;66;03m# At this stage the model is already loaded\u001b[39;00m\n\u001b[32m    557\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_quantized_and_base_model \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_peft_model(model) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_model_quantized_and_qat_trainable:\n\u001b[32m--> \u001b[39m\u001b[32m558\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    559\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou cannot perform fine-tuning on purely quantized models. Please attach trainable adapters on top of\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    560\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m the quantized model to correctly perform fine-tuning. Please see: https://huggingface.co/docs/transformers/peft\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    561\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m for more details\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    562\u001b[39m     )\n\u001b[32m    563\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m _is_quantized_and_base_model \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _quantization_method_supports_training:\n\u001b[32m    564\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    565\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe model you are trying to fine-tune is quantized with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel.hf_quantizer.quantization_config.quant_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    566\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m but that quantization method do not support training. Please open an issue on GitHub: https://github.com/huggingface/transformers\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    567\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m to request the support for training support for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel.hf_quantizer.quantization_config.quant_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    568\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: You cannot perform fine-tuning on purely quantized models. Please attach trainable adapters on top of the quantized model to correctly perform fine-tuning. Please see: https://huggingface.co/docs/transformers/peft for more details"
     ]
    }
   ],
   "source": [
    "# Create Trainer object\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c6857c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training, the model will be automatically saved to the Hub and the output directory\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588a5e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model again to the Hugging Face Hub\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338e5419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the log history\n",
    "log_history = trainer.state.log_history\n",
    "\n",
    "# Extract training / validation loss\n",
    "train_losses = [log[\"loss\"] for log in log_history if \"loss\" in log]\n",
    "epoch_train = [log[\"epoch\"] for log in log_history if \"loss\" in log]\n",
    "eval_losses = [log[\"eval_loss\"] for log in log_history if \"eval_loss\" in log]\n",
    "epoch_eval = [log[\"epoch\"] for log in log_history if \"eval_loss\" in log]\n",
    "\n",
    "# Plot the training loss\n",
    "plt.plot(epoch_train, train_losses, label=\"Training Loss\")\n",
    "plt.plot(epoch_eval, eval_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss per Epoch\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cb9543",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, tokenizer, pipe, trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14708bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = checkpoint_dir\n",
    "\n",
    "# Load Model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, torch_dtype=\"auto\", device_map=\"auto\", attn_implementation=\"eager\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a622a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and tokenizer into the pipeline\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "def test(test_sample):\n",
    "    # Convert as test example into a prompt with the Gemma template\n",
    "    prompt = pipe.tokenizer.apply_chat_template(\n",
    "        test_sample[\"messages\"][:1], tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    outputs = pipe(prompt, max_new_tokens=256, disable_compile=True)\n",
    "\n",
    "    # Extract the user query and original answer\n",
    "    print(f\"Question:\\n{test_sample['messages'][0]['content']}\")\n",
    "    print(f\"Original Answer:\\n{test_sample['messages'][1]['content']}\")\n",
    "    print(f\"Generated Answer:\\n{outputs[0]['generated_text'][len(prompt):].strip()}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "\n",
    "# Test with an unseen dataset\n",
    "for item in dataset['test']:\n",
    "    test(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88f7b56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
